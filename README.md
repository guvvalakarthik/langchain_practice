# ğŸš€ LangChain RAG with Ollama (VS Code Version)

This project implements a fully local **Retrieval-Augmented Generation (RAG)** pipeline using:

- ğŸ¦œ LangChain
- ğŸ¦™ Ollama (Llama3 / Llama2)
- ğŸ¤— HuggingFace Embeddings
- ğŸ” FAISS Vector Store
- âš¡ FastAPI (Backend)
- ğŸ–¥ï¸ VS Code (Development)

---

## ğŸ“Œ Project Overview

This system allows you to:

1. Load documents (PDF/Text)
2. Split text into chunks
3. Generate embeddings locally
4. Store vectors in FAISS
5. Retrieve relevant chunks
6. Use Ollama LLM to generate context-aware answers

âœ… 100% Local  
âœ… No OpenAI API required  
âœ… No cloud dependency  

---

## ğŸ—ï¸ Project Structure
